<p align="center">
  <img src="./ご提案.png" alt="ご提案イメージ" width="720">
</p>

# 分散GPU活用型 生成AI・CG基盤のご提案  
―― 遊休GPUを「ひとつのAIサービス」に変える仕組み

## 1. 背景：GPU不足と「遊休GPU」のギャップ

ここ数年、生成AIや高精細CGレンダリングの需要が急激に高まり、  
**クラウドGPUの価格高騰・リソース不足** が世界的な課題になっています。

一方で、多くの企業・クリエイター環境には、次のような「眠っているGPU資産」が存在します。

- 過去の開発や検証で導入したワークステーションやサーバー
- 一部の時間帯しか使われていない社内GPUマシン
- プロジェクト終了後、そのまま置かれている高性能GPUカード

これらのマシンは、**単体で見ると運用が面倒／使いにくい** ために活用されず、  
結果として「GPUは足りないのに、手元のGPUは遊んでいる」というギャップが生まれています。

本提案は、このギャップを埋めるための仕組みとして、  
**「社内・社外に分散したGPUをひとつのサービスとして束ねる基盤」** を提供するものです。

---

## 2. 本システムのコンセプト

### 「バラバラなGPUマシンを、ひとつのAIサービスにまとめる」

本システムは、複数のGPUサーバーやワークステーションを、  
**1つの統合API（エンドポイント）として見せるためのゲートウェイ層**です。

- 利用者視点では
  - 「1つのサーバーに対してリクエストを送るだけ」
- 実際の裏側では
  - 複数のGPUノードに自動で振り分けて処理を実行

という動作をします。

現在は ComfyUI（画像生成ワークフロー）向けに動作していますが、構造としては

- 高度なLLM推論（チャットボットや検索支援）
- 動画生成・動画変換パイプライン
- 3D/CGレンダリング
- 物理・数値シミュレーション

といった **GPUを使うさまざまなアプリケーションを同じ仕組みの上に載せることが可能**です。

---

## 3. 特徴とメリット

### 3-1. 遊休GPUを「サービス化」する

- 社内・グループ内に点在するGPUマシンを、この基盤に接続するだけで、  
  **共通のAIサービスの一部として再利用** できます。
- 各マシンは
  - オンプレミスサーバー
  - クラウドのGPUインスタンス
  - 開発者のワークステーション
  など、場所やOSが異なっていても構いません。
- これにより、
  - 「使われていないGPU」を**社内向け・社外向けの生成AIサービスへ変換**でき、
  - 高騰するクラウドGPU費用の一部を、自前リソースで吸収することができます。

### 3-2. 利用者には「1台のサーバー」として見える

- 利用側のアプリケーションは、  
  常に **同じURL／同じAPI形式** だけを叩けばよく、
  - どのGPUノードで処理されるかは意識する必要がありません。
- バックエンドでGPUが何台に増えても、あるいは一部がメンテナンス中でも、  
  **アプリ側のコード変更は不要**です。
- これにより、
  - フロントエンド開発者・業務システム側は、「AIサービスの使い方」に集中でき、
  - 「GPUの台数や構成をどうするか」というインフラ事情から切り離されます。

### 3-3. 段階的なスケールアウトが可能

- 小さく始めて、必要に応じてGPUノードを増やせます。
  - 例: 最初は1〜2台のGPUでスタート → 利用増加に合わせて順次追加。
- 基盤側でノード情報を追加するだけで、
  - 利用者から見えるAPIはそのまま、
  - 裏側だけが自然にスケールアウトします。
- これにより、
  - **初期投資を抑えつつ、利用状況を見ながら投資を段階的に拡大できる** ため、
  - ビジネスリスクをコントロールしやすくなります。

### 3-4. 異なる環境のGPUを混在させられる

- Linuxサーバーだけでなく、WindowsベースのGPUワークステーションも、  
  同じ基盤に参加させることが可能です。
- また、
  - オンプレミス環境
  - 各種クラウドプロバイダのGPUインスタンス
  を混在させた **ハイブリッド構成** も実現できます。
- 将来的には、
  - 軽いジョブはミドルレンジGPUへ、
  - 重いジョブはハイエンドGPUへ、
  といった**ジョブごとの自動振り分け**も視野に入り、  
  コスト効率の良いGPU運用が可能になります。

### 3-5. 運用の負担を最小化

- 各GPUノードは、セキュアなトンネル＋プロキシ経由で基盤に接続されるため、
  - リモートから直接ポート開放を行う必要がありません。
- ゲートウェイ側で
  - ノードの一時停止
  - 負荷の高いノードからの切り離し
  などの制御ができるため、
  - 個々のマシンにログインして調整する手間を大幅に削減できます。
- 実行ログやジョブ履歴をゲートウェイ側で集約できるため、
  - 「どのGPUでどのジョブがどれだけ動いたか」  
    といった情報を横断的に管理しやすくなります。

---

## 4. 想定される導入パターン

### パターンA：社内クリエイター向け 画像・動画生成基盤

- デザイナー／クリエイターが使う画像生成ツールや、  
  社内Webツールからの生成リクエストを **共通のAPI** に集約。
- 裏側では
  - 社内にあるワークステーションGPU
  - 時間帯によって空いている検証サーバー
  を束ね、リソースとして有効活用。

### パターンB：社内LLMサービスの共通基盤

- チャットボット、ナレッジ検索支援、コード補完ツールなど、  
  複数の社内システムが **同じLLM基盤** を利用。
- モデルやGPU構成は裏側で入れ替え・拡張しながら、  
  利用側は常に「同じ形のAPI」を使い続けられます。

### パターンC：クライアント向け生成AIサービスの基盤

- クライアント毎に個別のGPUインスタンスを立てるのではなく、  
  共同のGPUプール上でマルチテナント的に利用させる基盤として活用。
- 新規クライアントが増えた際も、
  - GPUノードを追加
  - ルーティングポリシーを調整
  するだけで、サービスの枠組みは共通のまま運用できます。

---

## 5. まとめ：  
### 「GPUが足りない」時代に、手元のGPUを最大限活かすための土台

- 市場全体では「GPU不足」と言われている一方で、  
  各社・各現場には **活かしきれていないGPUが確実に存在** します。
- 本基盤は、そうした **遊休GPU・分散GPUを“ひとつのAIサービス”として束ねるためのレイヤ** です。
- 一度このレイヤを整備してしまえば、
  - ComfyUIを使った画像生成
  - LLMによるテキスト生成
  - CGレンダリングやシミュレーション
  など、さまざまな生成AI／GPUアプリケーションを同じ仕組みの上に載せていくことができます。

結果として、

- 新しいAIサービスを立ち上げる際の **インフラ準備コストを大きく削減** し、
- 既存のGPU資産をビジネス価値に直結させる **「GPU運用の共通基盤」**

として活用していただけます。
